{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Tags",
    "colab": {
      "name": "classification_of_ASD_using_resting_state_fMRI.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZJcCGime2u9"
      },
      "source": [
        "#Prepare data for input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2C0UamJfQIB"
      },
      "source": [
        "## Setting 1: define data parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIPHD8KIZXq2"
      },
      "source": [
        "######################################\n",
        "# setting 1: define data parameters\n",
        "######################################\n",
        "\n",
        "desired_pipeline = 'cpac'\n",
        "desired_strategy = 'filt_global'\n",
        "desired_derivative = 'rois_cc400'\n",
        "download_data_dir = '/content/drive/MyDrive/abide'\n",
        "desired_age_max = 200.0\n",
        "desired_age_min = -1.0\n",
        "desired_site = None\n",
        "desired_sex = None\n",
        "desired_diagnosis = 'both'\n",
        "test_to_total_ratio = 0.1 # desired ratio of test data to all data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2y145SQbiRx"
      },
      "source": [
        "# calculate some needed variables\n",
        "\n",
        "data_main_path = download_data_dir  + '/'  'Outputs' + '/' + desired_pipeline + '/' + desired_strategy + '/' + desired_derivative\n",
        "corr_matrix_path = data_main_path + '.pkl'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUjYfYQNqfpy",
        "outputId": "d1ab1d4f-abef-4cb9-cc4e-cb4fab1322c7"
      },
      "source": [
        "#connect to google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mz8pMhDWXhF",
        "outputId": "7531d1a6-6512-4146-c303-07fd2d36f902"
      },
      "source": [
        "# install and load needed libraries\n",
        "\n",
        "!pip install PyPrind\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import pyprind\n",
        "import numpy.ma as ma\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting PyPrind\n",
            "  Downloading https://files.pythonhosted.org/packages/ab/b3/1f12ebc5009c65b607509393ad98240728b4401bc3593868fb161fdd3760/PyPrind-2.11.3-py2.py3-none-any.whl\n",
            "Installing collected packages: PyPrind\n",
            "Successfully installed PyPrind-2.11.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmiI0XN9mKT6"
      },
      "source": [
        "## Define helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5j0LLKPs3HYy"
      },
      "source": [
        "# this part of code is from https://github.com/preprocessed-connectomes-project/abide\n",
        "# for more info use this link to original repo\n",
        "\n",
        "\n",
        "# download_abide_preproc.py\n",
        "#\n",
        "# Author: Daniel Clark, 2015\n",
        "# Updated to python 3 and to support downloading by DX, Cameron Craddock, 2019\n",
        "\n",
        "\"\"\"\n",
        "This script downloads data from the Preprocessed Connetomes Project's\n",
        "ABIDE Preprocessed data release and stores the files in a local\n",
        "directory; users specify derivative, pipeline, strategy, and optionally\n",
        "age ranges, sex, site of interest\n",
        "\n",
        "Usage:\n",
        "    python download_abide_preproc.py -d <derivative> -p <pipeline>\n",
        "                                     -s <strategy> -o <out_dir>\n",
        "                                     [-lt <less_than>] [-gt <greater_than>]\n",
        "                                     [-x <sex>] [-t <site>]\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Main collect and download function\n",
        "def collect_and_download(derivative, pipeline, strategy, out_dir, less_than, greater_than, site, sex, diagnosis):\n",
        "    \"\"\"\n",
        "\n",
        "    Function to collect and download images from the ABIDE preprocessed\n",
        "    directory on FCP-INDI's S3 bucket\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    derivative : string\n",
        "        derivative or measure of interest\n",
        "    pipeline : string\n",
        "        pipeline used to process data of interest\n",
        "    strategy : string\n",
        "        noise removal strategy used to process data of interest\n",
        "    out_dir : string\n",
        "        filepath to a local directory to save files to\n",
        "    less_than : float\n",
        "        upper age (years) threshold for participants of interest\n",
        "    greater_than : float\n",
        "        lower age (years) threshold for participants of interest\n",
        "    site : string\n",
        "        acquisition site of interest\n",
        "    sex : string\n",
        "        'M' or 'F' to indicate whether to download male or female data\n",
        "    diagnosis : string\n",
        "        'asd', 'tdc', or 'both' corresponding to the diagnosis of the\n",
        "        participants for whom data should be downloaded\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "        this function does not return a value; it downloads data from\n",
        "        S3 to a local directory\n",
        "\n",
        "    :param derivative: \n",
        "    :param pipeline: \n",
        "    :param strategy: \n",
        "    :param out_dir: \n",
        "    :param less_than: \n",
        "    :param greater_than: \n",
        "    :param site: \n",
        "    :param sex:\n",
        "    :param diagnosis:\n",
        "    :return: \n",
        "    \"\"\"\n",
        "\n",
        "    # Import packages\n",
        "    import os\n",
        "    import urllib.request as request\n",
        "\n",
        "    # Init variables\n",
        "    mean_fd_thresh = 0.2\n",
        "    s3_prefix = 'https://s3.amazonaws.com/fcp-indi/data/Projects/'\\\n",
        "                'ABIDE_Initiative'\n",
        "    s3_pheno_path = '/'.join([s3_prefix, 'Phenotypic_V1_0b_preprocessed1.csv'])\n",
        "\n",
        "    # Format input arguments to be lower case, if not already\n",
        "    derivative = derivative.lower()\n",
        "    pipeline = pipeline.lower()\n",
        "    strategy = strategy.lower()\n",
        "\n",
        "    # Check derivative for extension\n",
        "    if 'roi' in derivative:\n",
        "        extension = '.1D'\n",
        "    else:\n",
        "        extension = '.nii.gz'\n",
        "\n",
        "    # If output path doesn't exist, create it\n",
        "    if not os.path.exists(out_dir):\n",
        "        print('Could not find {0}, creating now...'.format(out_dir))\n",
        "        os.makedirs(out_dir)\n",
        "\n",
        "    # Load the phenotype file from S3\n",
        "    s3_pheno_file = request.urlopen(s3_pheno_path)\n",
        "    pheno_list = s3_pheno_file.readlines()\n",
        "    print(pheno_list[0])\n",
        "\n",
        "    # Get header indices\n",
        "    header = pheno_list[0].decode().split(',')\n",
        "    try:\n",
        "        site_idx = header.index('SITE_ID')\n",
        "        file_idx = header.index('FILE_ID')\n",
        "        age_idx = header.index('AGE_AT_SCAN')\n",
        "        sex_idx = header.index('SEX')\n",
        "        dx_idx = header.index('DX_GROUP')\n",
        "        mean_fd_idx = header.index('func_mean_fd')\n",
        "    except Exception as exc:\n",
        "        err_msg = 'Unable to extract header information from the pheno file: {0}\\nHeader should have pheno info:' \\\n",
        "                  ' {1}\\nError: {2}'.format(s3_pheno_path, str(header), exc)\n",
        "        raise Exception(err_msg)\n",
        "\n",
        "    # Go through pheno file and build download paths\n",
        "    print('Collecting images of interest...')\n",
        "    s3_paths = []\n",
        "    for pheno_row in pheno_list[1:]:\n",
        "\n",
        "        # Comma separate the row\n",
        "        cs_row = pheno_row.decode().split(',')\n",
        "\n",
        "        try:\n",
        "            # See if it was preprocessed\n",
        "            row_file_id = cs_row[file_idx]\n",
        "            # Read in participant info\n",
        "            row_site = cs_row[site_idx]\n",
        "            row_age = float(cs_row[age_idx])\n",
        "            row_sex = cs_row[sex_idx]\n",
        "            row_dx = cs_row[dx_idx]\n",
        "            row_mean_fd = float(cs_row[mean_fd_idx])\n",
        "        except Exception as e:\n",
        "            err_msg = 'Error extracting info from phenotypic file, skipping...'\n",
        "            print(err_msg)\n",
        "            continue\n",
        "\n",
        "        # If the filename isn't specified, skip\n",
        "        if row_file_id == 'no_filename':\n",
        "            continue\n",
        "        # If mean fd is too large, skip\n",
        "        if row_mean_fd >= mean_fd_thresh:\n",
        "            continue\n",
        "\n",
        "        # Test phenotypic criteria (three if's looks cleaner than one long if)\n",
        "        # Test sex\n",
        "        if (sex == 'M' and row_sex != '1') or (sex == 'F' and row_sex != '2'):\n",
        "            continue\n",
        "\n",
        "        if (diagnosis == 'asd' and row_dx != '1') or (diagnosis == 'tdc' and row_dx != '2'):\n",
        "            continue\n",
        "\n",
        "        # Test site\n",
        "        if site is not None and site.lower() != row_site.lower():\n",
        "            continue\n",
        "        # Test age range\n",
        "        if greater_than < row_age < less_than:\n",
        "            filename = row_file_id + '_' + derivative + extension\n",
        "            s3_path = '/'.join([s3_prefix, 'Outputs', pipeline, strategy, derivative, filename])\n",
        "            print('Adding {0} to download queue...'.format(s3_path))\n",
        "            s3_paths.append(s3_path)\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "    # And download the items\n",
        "    total_num_files = len(s3_paths)\n",
        "    for path_idx, s3_path in enumerate(s3_paths):\n",
        "        rel_path = s3_path.lstrip(s3_prefix)\n",
        "        download_file = os.path.join(out_dir, rel_path)\n",
        "        download_dir = os.path.dirname(download_file)\n",
        "        if not os.path.exists(download_dir):\n",
        "            os.makedirs(download_dir)\n",
        "        try:\n",
        "            if not os.path.exists(download_file):\n",
        "                print('Retrieving: {0}'.format(download_file))\n",
        "                request.urlretrieve(s3_path, download_file)\n",
        "                print('{0:3f}% percent complete'.format(100*(float(path_idx+1)/total_num_files)))\n",
        "            else:\n",
        "                print('File {0} already exists, skipping...'.format(download_file))\n",
        "        except Exception as exc:\n",
        "            print('There was a problem downloading {0}.\\n Check input arguments and try again.'.format(s3_path))\n",
        "\n",
        "    # Print all done\n",
        "    print('Done!')\n",
        "\n",
        "\n",
        "# Make module executable\n",
        "# if __name__ == '__main__':\n",
        "\n",
        "#     # Import packages\n",
        "#     import argparse\n",
        "#     import os\n",
        "#     import sys\n",
        "\n",
        "#     # Init argument parser\n",
        "#     parser = argparse.ArgumentParser(description=__doc__)\n",
        "\n",
        "#     # Required arguments\n",
        "#     parser.add_argument('-a', '--asd', required=False, default=False, action='store_true',\n",
        "#                         help='Only download data for participants with ASD.'\n",
        "#                              ' Specifying neither or both -a and -c will download data from all participants.')\n",
        "#     parser.add_argument('-c', '--tdc', required=False, default=False, action='store_true',\n",
        "#                         help='Only download data for participants who are typically developing controls.'\n",
        "#                              ' Specifying neither or both -a and -c will download data from all participants.')\n",
        "#     parser.add_argument('-d', '--derivative', nargs=1, required=True, type=str,\n",
        "#                         help='Derivative of interest (e.g. \\'reho\\')')\n",
        "#     parser.add_argument('-p', '--pipeline', nargs=1, required=True, type=str,\n",
        "#                         help='Pipeline used to preprocess the data (e.g. \\'cpac\\')')\n",
        "#     parser.add_argument('-s', '--strategy', nargs=1, required=True, type=str,\n",
        "#                         help='Noise-removal strategy used during preprocessing (e.g. \\'nofilt_noglobal\\'')\n",
        "#     parser.add_argument('-o', '--out_dir', nargs=1, required=True, type=str,\n",
        "#                         help='Path to local folder to download files to')\n",
        "\n",
        "#     # Optional arguments\n",
        "#     parser.add_argument('-lt', '--less_than', nargs=1, required=False,\n",
        "#                         type=float, help='Upper age threshold (in years) of participants to download (e.g. for '\n",
        "#                                          'subjects 30 or younger, \\'-lt 31\\')')\n",
        "#     parser.add_argument('-gt', '--greater_than', nargs=1, required=False,\n",
        "#                         type=int, help='Lower age threshold (in years) of participants to download (e.g. for '\n",
        "#                                        'subjects 31 or older, \\'-gt 30\\')')\n",
        "#     parser.add_argument('-t', '--site', nargs=1, required=False, type=str,\n",
        "#                         help='Site of interest to download from (e.g. \\'Caltech\\'')\n",
        "#     parser.add_argument('-x', '--sex', nargs=1, required=False, type=str,\n",
        "#                         help='Participant sex of interest to download only (e.g. \\'M\\' or \\'F\\')')\n",
        "\n",
        "#     # Parse and gather arguments\n",
        "#     args = parser.parse_args()\n",
        "\n",
        "#     # Init variables\n",
        "#     desired_derivative = args.derivative[0].lower()\n",
        "#     desired_pipeline = args.pipeline[0].lower()\n",
        "#     desired_strategy = args.strategy[0].lower()\n",
        "#     download_data_dir = os.path.abspath(args.out_dir[0])\n",
        "\n",
        "#     # Try and init optional arguments\n",
        "\n",
        "#     # for diagnosis if both ASD and TDC flags are set to true or false, we download both\n",
        "#     desired_diagnosis = ''\n",
        "#     if args.tdc == args.asd:\n",
        "#         desired_diagnosis = 'both'\n",
        "#         print('Downloading data for ASD and TDC participants')\n",
        "#     elif args.tdc:\n",
        "#         desired_diagnosis = 'tdc'\n",
        "#         print('Downloading data for TDC participants')\n",
        "#     elif args.asd:\n",
        "#         desired_diagnosis = 'asd'\n",
        "#         print('Downloading data for ASD participants')\n",
        "\n",
        "#     try:\n",
        "#         desired_age_max = args.less_than[0]\n",
        "#         print('Using upper age threshold of {0:d}...'.format(desired_age_max))\n",
        "#     except TypeError:\n",
        "#         desired_age_max = 200.0\n",
        "#         print('No upper age threshold specified')\n",
        "\n",
        "#     try:\n",
        "#         desired_age_min = args.greater_than[0]\n",
        "#         print('Using lower age threshold of {0:d}...'.format(desired_age_min))\n",
        "#     except TypeError:\n",
        "#         desired_age_min = -1.0\n",
        "#         print('No lower age threshold specified')\n",
        "\n",
        "#     try:\n",
        "#         desired_site = args.site[0]\n",
        "#     except TypeError:\n",
        "#         desired_site = None\n",
        "#         print('No site specified, using all sites...')\n",
        "\n",
        "#     try:\n",
        "#         desired_sex = args.sex[0].upper()\n",
        "#         if desired_sex == 'M':\n",
        "#             print('Downloading only male subjects...')\n",
        "#         elif desired_sex == 'F':\n",
        "#             print('Downloading only female subjects...')\n",
        "#         else:\n",
        "#             print('Please specify \\'M\\' or \\'F\\' for sex and try again')\n",
        "#             sys.exit()\n",
        "#     except TypeError:\n",
        "#         desired_sex = None\n",
        "#         print('No sex specified, using all sexes...')\n",
        "\n",
        "#     # Call the collect and download routine\n",
        "#     collect_and_download(desired_derivative, desired_pipeline, desired_strategy, download_data_dir, desired_age_max,\n",
        "#                          desired_age_min, desired_site, desired_sex, desired_diagnosis)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5txlivMFWXip"
      },
      "source": [
        "# some helper function\n",
        "\n",
        "def get_key(filename):\n",
        "  f_split = filename.split('_')\n",
        "  if f_split[3] == 'rois':\n",
        "    key = '_'.join(f_split[0:3]) \n",
        "  else:\n",
        "    key = '_'.join(f_split[0:2])\n",
        "  return key\n",
        "\n",
        "\n",
        "def get_label(filename):\n",
        "  assert (filename in labels)\n",
        "  return labels[filename]\n",
        "\n",
        "\n",
        "def get_s_label(filename):\n",
        "  assert (filename in s_labels)\n",
        "  return s_labels[filename]\n",
        "\n",
        "\n",
        "def get_a_label(filename):\n",
        "  assert (filename in a_labels)\n",
        "  return a_labels[filename]\n",
        "\n",
        "\n",
        "def get_corr_data(filename):\n",
        "  for file in os.listdir(data_main_path):\n",
        "    if file.startswith(filename):\n",
        "      df = pd.read_csv(os.path.join(data_main_path, file), sep='\\t')\n",
        "          \n",
        "  with np.errstate(invalid=\"ignore\"):\n",
        "    corr = np.nan_to_num(np.corrcoef(df.T))\n",
        "    mask = np.invert(np.tri(corr.shape[0], k=-1, dtype=bool))\n",
        "    m = ma.masked_where(mask == 1, mask)\n",
        "    return ma.masked_where(m, corr).compressed()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHqUqZWsmlQZ"
      },
      "source": [
        "## Load or make correlation matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJkIgX69pVrV"
      },
      "source": [
        "# Load or make correlation matrix\n",
        "\n",
        "if not os.path.exists(corr_matrix_path):\n",
        "  if not os.path.exists(data_main_path):\n",
        "    print('Downloading preprocessed data')\n",
        "    collect_and_download(desired_derivative, desired_pipeline, desired_strategy, download_data_dir, desired_age_max, desired_age_min, desired_site, desired_sex, desired_diagnosis)\n",
        "  \n",
        "  print('Making correlation matrix')\n",
        "  flist = os.listdir(data_main_path)\n",
        "  print('there are ' + str(len(flist)) + ' files')\n",
        "\n",
        "  for f in range(len(flist)):\n",
        "      flist[f] = get_key(flist[f])\n",
        "      \n",
        "\n",
        "  df_labels = pd.read_csv('https://s3.amazonaws.com/fcp-indi/data/Projects/ABIDE_Initiative/Phenotypic_V1_0b_preprocessed1.csv')\n",
        "\n",
        "  df_labels.DX_GROUP = df_labels.DX_GROUP.map({1: 1, 2:0})\n",
        "  print('there are ' + str(len(df_labels)) + ' labels')\n",
        "\n",
        "  labels = {}\n",
        "  for row in df_labels.iterrows():\n",
        "      file_id = row[1]['FILE_ID']\n",
        "      y_label = row[1]['DX_GROUP']\n",
        "      if file_id == 'no_filename':\n",
        "          continue\n",
        "      assert(file_id not in labels)\n",
        "      labels[file_id] = y_label\n",
        "  \n",
        "  s_labels = {}\n",
        "  for row in df_labels.iterrows():\n",
        "      file_id = row[1]['FILE_ID']\n",
        "      sex_label = row[1]['SEX']\n",
        "      if file_id == 'no_filename':\n",
        "          continue\n",
        "      assert(file_id not in s_labels)\n",
        "      s_labels[file_id] = sex_label\n",
        "\n",
        "  a_labels = {}\n",
        "  for row in df_labels.iterrows():\n",
        "      file_id = row[1]['FILE_ID']\n",
        "      age_lable = row[1]['AGE_AT_SCAN']\n",
        "      if file_id == 'no_filename':\n",
        "          continue\n",
        "      assert(file_id not in a_labels)\n",
        "      a_labels[file_id] = age_lable\n",
        "\n",
        "  #make corr matrix\n",
        "  pbar=pyprind.ProgBar(len(flist))\n",
        "  all_corr = {}\n",
        "  for f in flist:\n",
        "    lab = get_label(f)\n",
        "    s_lab = get_s_label(f)\n",
        "    a_lab = get_a_label(f)\n",
        "    all_corr[f] = (get_corr_data(f), lab, s_lab, a_lab)\n",
        "    pbar.update()\n",
        "\n",
        "  print('Corr-computations finished')\n",
        "  print('total labels & file: ' + str(len(all_corr)))\n",
        "  pickle.dump(all_corr, open(corr_matrix_path , 'wb'))\n",
        "  print('Saving to file finished')\n",
        "else:\n",
        "  all_corr = pickle.load(open(corr_matrix_path, 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cHkrjLMziZa"
      },
      "source": [
        "## Make x_train, y_train, x_test and *y_test* as input for NN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RptuDKC-kMIz",
        "outputId": "4ee8eaf2-f29d-41f1-e083-12cb36d550b4"
      },
      "source": [
        "# Make x_train, y_train, x_test and *y_test* as input for NN model\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X = []\n",
        "for x_case_counter in all_corr:\n",
        "  X.append(list(all_corr[x_case_counter][0]))\n",
        "X = pd.DataFrame(X)\n",
        "sc = StandardScaler()\n",
        "X = sc.fit_transform(X)\n",
        "Y = []\n",
        "for y_case_counter in all_corr:\n",
        "  Y.append(all_corr[y_case_counter][1])\n",
        "Y = np.asarray(Y)\n",
        "\n",
        "print('Shape of all data as X: ' + str(np.shape(X)))\n",
        "print('Shape of all data as Y: ' + str(np.shape(Y)))\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=test_to_total_ratio)\n",
        "\n",
        "print('Shape of x_train: ' + str(np.shape(x_train)))\n",
        "print('Shape of y_train: ' + str(np.shape(y_train)))\n",
        "print('Shape of x_test: ' + str(np.shape(x_test)))\n",
        "print('Shape of y_test: ' + str(np.shape(y_test)))\n",
        "\n",
        "input_X_dimension = np.shape(x_train)[1]\n",
        "print('input X dimension: ' + str(input_X_dimension))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of all data as X: (884, 76636)\n",
            "Shape of all data as Y: (884,)\n",
            "Shape of x_train: (795, 76636)\n",
            "Shape of y_train: (795,)\n",
            "Shape of x_test: (89, 76636)\n",
            "Shape of y_test: (89,)\n",
            "input X dimension: 76636\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6ELIsV1gPhD"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6xpmxafuNoT"
      },
      "source": [
        "## Setting 2: Make model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgqSWe_ekSPh",
        "outputId": "fd89e7a3-b2d9-48d6-e247-ffa8795af72a"
      },
      "source": [
        "#make a model\n",
        "\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "input_X_dimension\n",
        "\n",
        "classifier = Sequential()\n",
        "classifier.add(Dense(8, activation='relu', kernel_initializer='random_normal', input_dim = input_X_dimension))\n",
        "classifier.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))\n",
        "loss_fn = tf.keras.losses.MeanSquaredError(reduction='sum_over_batch_size')\n",
        "classifier.summary()\n",
        "\n",
        "classifier.compile(optimizer ='adam',loss=loss_fn, metrics =['accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_8 (Dense)              (None, 8)                 613096    \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 613,105\n",
            "Trainable params: 613,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfvCOGx6uZDp"
      },
      "source": [
        "## Setting 3: fit model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 945
        },
        "id": "7F61yKNBkViJ",
        "outputId": "1743553e-8b89-4563-98d0-0a547ed9baeb"
      },
      "source": [
        "#fit model\n",
        "\n",
        "accuracy = []\n",
        "F1 = []\n",
        "history = classifier.fit(x_train,y_train, batch_size=40, epochs= 20 , validation_data = (x_test, y_test))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "20/20 [==============================] - 1s 33ms/step - loss: 0.2606 - accuracy: 0.5834 - val_loss: 0.1958 - val_accuracy: 0.6517\n",
            "Epoch 2/20\n",
            "20/20 [==============================] - 0s 21ms/step - loss: 0.0789 - accuracy: 0.9115 - val_loss: 0.2140 - val_accuracy: 0.7079\n",
            "Epoch 3/20\n",
            "20/20 [==============================] - 0s 20ms/step - loss: 0.0246 - accuracy: 0.9789 - val_loss: 0.2227 - val_accuracy: 0.6742\n",
            "Epoch 4/20\n",
            "20/20 [==============================] - 0s 20ms/step - loss: 0.0103 - accuracy: 0.9829 - val_loss: 0.2259 - val_accuracy: 0.6854\n",
            "Epoch 5/20\n",
            "20/20 [==============================] - 0s 20ms/step - loss: 0.0078 - accuracy: 0.9861 - val_loss: 0.2294 - val_accuracy: 0.6854\n",
            "Epoch 6/20\n",
            "20/20 [==============================] - 0s 20ms/step - loss: 0.0049 - accuracy: 0.9895 - val_loss: 0.2311 - val_accuracy: 0.6629\n",
            "Epoch 7/20\n",
            "20/20 [==============================] - 0s 21ms/step - loss: 0.0053 - accuracy: 0.9919 - val_loss: 0.2303 - val_accuracy: 0.6517\n",
            "Epoch 8/20\n",
            "20/20 [==============================] - 0s 20ms/step - loss: 0.0065 - accuracy: 0.9882 - val_loss: 0.2313 - val_accuracy: 0.6629\n",
            "Epoch 9/20\n",
            "20/20 [==============================] - 0s 21ms/step - loss: 0.0048 - accuracy: 0.9899 - val_loss: 0.2327 - val_accuracy: 0.6629\n",
            "Epoch 10/20\n",
            "20/20 [==============================] - 0s 21ms/step - loss: 0.0032 - accuracy: 0.9932 - val_loss: 0.2341 - val_accuracy: 0.6629\n",
            "Epoch 11/20\n",
            "20/20 [==============================] - 0s 20ms/step - loss: 0.0045 - accuracy: 0.9917 - val_loss: 0.2403 - val_accuracy: 0.6629\n",
            "Epoch 12/20\n",
            "20/20 [==============================] - 0s 21ms/step - loss: 0.0042 - accuracy: 0.9946 - val_loss: 0.2412 - val_accuracy: 0.6629\n",
            "Epoch 13/20\n",
            "20/20 [==============================] - 0s 20ms/step - loss: 0.0031 - accuracy: 0.9929 - val_loss: 0.2415 - val_accuracy: 0.6517\n",
            "Epoch 14/20\n",
            "20/20 [==============================] - 0s 21ms/step - loss: 0.0038 - accuracy: 0.9926 - val_loss: 0.2423 - val_accuracy: 0.6517\n",
            "Epoch 15/20\n",
            "20/20 [==============================] - 0s 22ms/step - loss: 0.0055 - accuracy: 0.9890 - val_loss: 0.2433 - val_accuracy: 0.6517\n",
            "Epoch 16/20\n",
            "20/20 [==============================] - 0s 20ms/step - loss: 0.0047 - accuracy: 0.9931 - val_loss: 0.2439 - val_accuracy: 0.6517\n",
            "Epoch 17/20\n",
            "20/20 [==============================] - 0s 21ms/step - loss: 0.0042 - accuracy: 0.9901 - val_loss: 0.2444 - val_accuracy: 0.6629\n",
            "Epoch 18/20\n",
            "20/20 [==============================] - 0s 21ms/step - loss: 0.0045 - accuracy: 0.9917 - val_loss: 0.2450 - val_accuracy: 0.6629\n",
            "Epoch 19/20\n",
            "20/20 [==============================] - 0s 21ms/step - loss: 0.0049 - accuracy: 0.9896 - val_loss: 0.2454 - val_accuracy: 0.6629\n",
            "Epoch 20/20\n",
            "20/20 [==============================] - 0s 20ms/step - loss: 0.0047 - accuracy: 0.9923 - val_loss: 0.2458 - val_accuracy: 0.6629\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhLzCYFQkQFx",
        "outputId": "5304b9c5-4eef-419a-aac6-c27cdec9d228"
      },
      "source": [
        "S = []\n",
        "for ll in rest:\n",
        "  S.append(rest[ll][2])\n",
        "S = np.asarray(S)\n",
        "\n",
        "A = []\n",
        "for nn in rest:\n",
        "  A.append(rest[nn][3])\n",
        "A = np.asarray(A)\n",
        "\n",
        "print(np.shape(S))\n",
        "print(np.shape(A))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(88,)\n",
            "(88,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-x_WA3dKkabZ"
      },
      "source": [
        "Men = {}\n",
        "Women = {}\n",
        "for item in rest:\n",
        "  if(rest[item][2] == 1):\n",
        "    Men[item] = rest[item]\n",
        "  else:\n",
        "    Women[item] = rest[item]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inNMoU8Skb4h"
      },
      "source": [
        "len(Women)\n",
        "# Men"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5ikgXXokfPR"
      },
      "source": [
        "Xmen = []\n",
        "for ff in Men:\n",
        "  Xmen.append(list(Men[ff][0]))\n",
        "Xmen = pd.DataFrame(Xmen)\n",
        "sc = StandardScaler()\n",
        "Xmen = sc.fit_transform(Xmen)\n",
        "Ymen = []\n",
        "for jj in Men:\n",
        "  Ymen.append(Men[jj][1])\n",
        "Ymen = np.asarray(Ymen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBM9p5bikgzB"
      },
      "source": [
        "np.shape(Ymen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOGrd8FPkhZp"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "y_pred=classifier.predict(Xmen)\n",
        "\n",
        "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
        "\n",
        "cm = confusion_matrix(Ymen, y_pred)\n",
        "accuracy.append((cm[0][0] + cm[1][1])/sum(sum(cm)))\n",
        "precision = cm[0][0]/(cm[0][0] + cm[1][1])\n",
        "recall =  cm[0][0]/(cm[0][0] + cm[1][0])\n",
        "F1.append(2 * precision * recall / (precision + recall))\n",
        "\n",
        "print('accuracy =', np.mean(accuracy))\n",
        "print('F1 =', np.mean(F1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2riBCMfkjDp"
      },
      "source": [
        "#calculate accuracy & f1 in test dataset\n",
        "y_pred=classifier.predict(x_test)\n",
        "\n",
        "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "accuracy.append((cm[0][0] + cm[1][1])/sum(sum(cm)))\n",
        "precision = cm[0][0]/(cm[0][0] + cm[1][1])\n",
        "recall =  cm[0][0]/(cm[0][0] + cm[1][0])\n",
        "F1.append(2 * precision * recall / (precision + recall))\n",
        "\n",
        "print('accuracy =', np.mean(accuracy))\n",
        "print('F1 =', np.mean(F1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFWxAL-qkoWw"
      },
      "source": [
        "#get weights from model\n",
        "for layer in classifier.layers:\n",
        "    weights = layer.get_weights() # list of numpy arrays"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFh5Jigl4wMO"
      },
      "source": [
        "# calculate normal-autistic corr-diff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RU2udgsS4vff"
      },
      "source": [
        "#divide whole collection to normal and autistic\n",
        "normals = {}\n",
        "autistic = {}\n",
        "for item in all_corr:\n",
        "  if(all_corr[item][1] == 0):\n",
        "    normals[item] = all_corr[item]\n",
        "  else:\n",
        "    autistic[item] = all_corr[item]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3rCtacU48le"
      },
      "source": [
        "#sum over nomal\n",
        "sum_n = np.zeros(19900)\n",
        "for item_n in normals:\n",
        "  sum_n = sum_n + normals[item_n][0]\n",
        "\n",
        "#sum over autistic\n",
        "sum_a = np.zeros(19900)\n",
        "for item_a in autistic:\n",
        "  sum_a = sum_a + autistic[item_a][0]\n",
        "\n",
        "print(np.shape(sum_n))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Xq1Cbkq5APd"
      },
      "source": [
        "#make diff matrix\n",
        "diff = sum_n - sum_a\n",
        "print(np.shape(diff))\n",
        "print(sum_n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_z2zjSc5CFm"
      },
      "source": [
        "#add index to diff matrix\n",
        "num_diff = []\n",
        "for counter in range(len(diff)):\n",
        "  num_diff = num_diff + [[counter ,diff[counter]]]\n",
        "\n",
        "print(num_diff)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7_Rczfs5EFF"
      },
      "source": [
        "#sort diff matrix\n",
        "arr = np.array(num_diff)\n",
        "sorted_diff = arr[arr[:,1].argsort()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOLNMFJZ5FxV"
      },
      "source": [
        "#make a mask from first and last 1000 elements\n",
        "mask1 = sorted_diff[0:1000,0]\n",
        "mask2 = sorted_diff[-1000:,0]\n",
        "mask = np.concatenate((mask1, mask2))\n",
        "mask = mask.astype('int32')\n",
        "print(mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYEBCwlb5H3t"
      },
      "source": [
        "# apply mask to data\n",
        "masked_diff = np.array(num_diff)[mask.astype(int)]\n",
        "print(masked_diff)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVVZaEeZ5J59"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X = []\n",
        "for ff in all_corr:\n",
        "  tempx = all_corr[ff][0]\n",
        "  masked_tempx = np.array(tempx)[mask.astype(int)]\n",
        "  X.append(list(masked_tempx))\n",
        "X = pd.DataFrame(X)\n",
        "sc = StandardScaler()\n",
        "X = sc.fit_transform(X)\n",
        "Y = []\n",
        "for jj in all_corr:\n",
        "  Y.append(all_corr[jj][1])\n",
        "Y = np.asarray(Y)\n",
        "\n",
        "print(np.shape(X))\n",
        "print(np.shape(Y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yNtnkiv5L5l"
      },
      "source": [
        "import keras\n",
        "classifier = Sequential()\n",
        "classifier.add(Dense(100, activation='relu', kernel_initializer='random_normal', input_dim=2000))\n",
        "classifier.add(Dense(4, activation='relu', kernel_initializer='random_normal'))\n",
        "classifier.add(Dense(2, activation='relu', kernel_initializer='random_normal'))\n",
        "classifier.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))\n",
        "loss_fn = tf.keras.losses.MeanSquaredError(reduction='sum_over_batch_size')\n",
        "\n",
        "classifier.compile(optimizer ='adam',loss=loss_fn, metrics =['accuracy'])\n",
        "\n",
        "accuracy = []\n",
        "F1 = []\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjRoq6HD5Nj-"
      },
      "source": [
        "classifier.fit(x_train,y_train, batch_size = 40, epochs = 100)\n",
        "y_pred=classifier.predict(x_test)\n",
        "\n",
        "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "accuracy.append((cm[0][0] + cm[1][1])/sum(sum(cm)))\n",
        "precision = cm[0][0]/(cm[0][0] + cm[1][1])\n",
        "recall =  cm[0][0]/(cm[0][0] + cm[1][0])\n",
        "F1.append(2 * precision * recall / (precision + recall))\n",
        "\n",
        "print('accuracy =', np.mean(accuracy))\n",
        "print('F1 =', np.mean(F1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHRsHot75P5d"
      },
      "source": [
        "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "accuracy.append((cm[0][0] + cm[1][1])/sum(sum(cm)))\n",
        "precision = cm[0][0]/(cm[0][0] + cm[1][1])\n",
        "recall =  cm[0][0]/(cm[0][0] + cm[1][0])\n",
        "F1.append(2 * precision * recall / (precision + recall))\n",
        "\n",
        "print('accuracy =', np.mean(accuracy))\n",
        "print('F1 =', np.mean(F1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3eBBibf5Ri1"
      },
      "source": [
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}